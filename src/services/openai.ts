import OpenAI, { AzureOpenAI } from "openai";
import {
  ChatCompletionMessageParam,
  ChatCompletionTool,
} from "openai/resources";

import { kEnvs, cozeConfig } from "../utils/env"; 
import { withDefault } from "../utils/base";
import { ChatCompletionCreateParamsBase } from "openai/resources/chat/completions";
import { Logger } from "../utils/log";
import { kProxyAgent } from "./proxy";
import { isNotEmpty } from "../utils/is";

export interface ChatOptions {
  user?: string;  // ä¿®æ”¹ä¸ºå¯é€‰
  system?: string;
  jsonMode?: boolean;
  requestId?: string;
  trace?: boolean;
  onStream?: (text: string) => void;  // ç¡®ä¿è¿™ä¸ªå‚æ•°åœ¨æ¥å£ä¸­å®šä¹‰
  tools?: ChatCompletionTool[];
  model?: string;
  enableSearch?: boolean;
}

class OpenAIClient {
  traceInput = false;
  traceOutput = true;
  private _logger = Logger.create({ tag: "Open AI" });

  deployment?: string;

  private _client?: OpenAI;
  private _init() {
    this.deployment = kEnvs.AZURE_OPENAI_DEPLOYMENT;
    if (!this._client) {
      this._logger.log('åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯:', {
        model: kEnvs.OPENAI_MODEL,
        baseURL: kEnvs.OPENAI_BASE_URL,
        useCoze: cozeConfig.enabled
      });
      
      // å¦‚æœå¯ç”¨äº† Cozeï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„å®¢æˆ·ç«¯å®ä¾‹
      if (cozeConfig.enabled) {
        this._client = new OpenAI({ 
          httpAgent: kProxyAgent,
          baseURL: cozeConfig.baseUrl
        });
        return;
      }

      this._client = kEnvs.AZURE_OPENAI_API_KEY
        ? new AzureOpenAI({
            httpAgent: kProxyAgent,
            deployment: this.deployment,
          })
        : new OpenAI({ 
            httpAgent: kProxyAgent,
            baseURL: kEnvs.OPENAI_BASE_URL
          });
    }
  }

  private _abortCallbacks: Record<string, VoidFunction> = {
    // requestId: abortStreamCallback
  };

  cancel(requestId: string) {
    this._init();
    if (this._abortCallbacks[requestId]) {
      this._abortCallbacks[requestId]();
      delete this._abortCallbacks[requestId];
    }
  }

  async chat(options: ChatOptions) {
    this._init();
    
    // å¦‚æœå¯ç”¨äº† Cozeï¼Œä½¿ç”¨ Coze é€‚é…å™¨
    if (cozeConfig.enabled) {
      try {
        // ä½¿ç”¨ coze å•ä¾‹å®ä¾‹ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°å®ä¾‹
        const { coze } = require('./coze');
        this._logger.log('ä½¿ç”¨ Coze é€‚é…å™¨å¤„ç†è¯·æ±‚');
        return await coze.chat(options);
      } catch (error) {
        this._logger.error('Coze é€‚é…å™¨é”™è¯¯:', error);
        return null;
      }
    }
    let {
      user,
      system,
      tools,
      jsonMode,
      requestId,
      trace = false,
      model = this.deployment ?? kEnvs.OPENAI_MODEL ?? "gpt-4o",
    } = options;
    if (trace && this.traceInput) {
      this._logger.log(
        `ğŸ”¥ onAskAI\nğŸ¤–ï¸ System: ${system ?? "None"}\nğŸ˜Š User: ${user}`.trim()
      );
    }
    const systemMsg: ChatCompletionMessageParam[] = isNotEmpty(system)
      ? [{ role: "system", content: system! }]
      : [];
    let signal: AbortSignal | undefined;
    if (requestId) {
      const controller = new AbortController();
      this._abortCallbacks[requestId] = () => controller.abort();
      signal = controller.signal;
    }
    const chatCompletion = await this._client!.chat.completions.create(
      {
        model,
        tools,
        messages: [...systemMsg, { role: "user", content: user || '' }], // ä¿®å¤ï¼šæ·»åŠ é»˜è®¤ç©ºå­—ç¬¦ä¸²
        response_format: jsonMode ? { type: "json_object" } : undefined,
      },
      { signal }
    ).catch((e) => {
      this._logger.error("LLM å“åº”å¼‚å¸¸", e);
      return null;
    });
    if (requestId) {
      delete this._abortCallbacks[requestId];
    }
    const message = chatCompletion?.choices?.[0]?.message;
    if (trace && this.traceOutput) {
      this._logger.log(`âœ… Answer: ${message?.content ?? "None"}`.trim());
    }
    return message;
  }

  async chatStream(options: ChatOptions) {
    this._init();
    
    // å¦‚æœå¯ç”¨äº† Cozeï¼Œä½¿ç”¨ Coze é€‚é…å™¨çš„æµå¼å“åº”
    if (cozeConfig.enabled) {
      try {
        // ç›´æ¥å¯¼å…¥ coze å•ä¾‹
        const { coze } = require('./coze');
        this._logger.log('ä½¿ç”¨ Coze é€‚é…å™¨å¤„ç†æµå¼è¯·æ±‚');
        
        // ç¡®ä¿ onStream å›è°ƒå‡½æ•°èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ–‡æœ¬åˆ†æ®µ
        const originalOnStream = options.onStream;
        let buffer = '';
        
        // åˆ›å»ºæ–°çš„ onStream å¤„ç†å‡½æ•°ï¼Œå®ç°ç±»ä¼¼æµ‹è¯•æ–‡ä»¶ä¸­çš„æ–‡æœ¬åˆ†æ®µé€»è¾‘
        options.onStream = (text) => {
          buffer += text;
          
          // å½“é‡åˆ°å¥å­ç»“æŸç¬¦å·æ—¶æ‰è°ƒç”¨åŸå§‹çš„ onStream
          if (/[ã€‚ï¼ï¼Ÿ]/.test(text) && buffer.trim()) {
            if (originalOnStream) {
              originalOnStream(buffer);
            }
            buffer = '';
          }
        };
        
        // è°ƒç”¨ coze çš„æµå¼å“åº”æ–¹æ³•
        const result = await coze.chatStream(options);
        
        // å¤„ç†å¯èƒ½å‰©ä½™çš„æ–‡æœ¬
        if (buffer.trim() && originalOnStream) {
          originalOnStream(buffer);
        }
        
        return result;
      } catch (error) {
        this._logger.error('Coze é€‚é…å™¨æµå¼è¯·æ±‚é”™è¯¯:', error);
        return null;
      }
    }
    
    // åŸç”Ÿ OpenAI å¤„ç†é€»è¾‘ä¿æŒä¸å˜
    let {
      user,
      system,
      tools,
      jsonMode,
      requestId,
      onStream,
      trace = false,
      model = this.deployment ?? kEnvs.OPENAI_MODEL ?? "gpt-4o",
      enableSearch = kEnvs.QWEN_ENABLE_SEARCH,
    } = options;
    
    if (trace && this.traceInput) {
      this._logger.log(
        `ğŸ”¥ onAskAI\nğŸ¤–ï¸ System: ${system ?? "None"}\nğŸ˜Š User: ${user}`.trim()
      );
    }
    const systemMsg: ChatCompletionMessageParam[] = isNotEmpty(system)
      ? [{ role: "system", content: system! }]
      : [];
    const stream = await this._client!.chat.completions
      .create({
        model,
        tools,
        stream: true,
        messages: [...systemMsg, { role: "user", content: user || '' }], // ä¿®å¤ undefined é—®é¢˜
        response_format: jsonMode ? { type: "json_object" } : undefined,
        ...(enableSearch && { enable_search: true })
      })
      .catch((e) => {
        this._logger.error("LLM å“åº”å¼‚å¸¸", e);
        return null;
      });
    if (!stream) {
      return;
    }
    if (requestId) {
      this._abortCallbacks[requestId] = () => stream.controller.abort();
    }
    let content = "";
    for await (const chunk of stream) {
      const text = chunk.choices[0]?.delta?.content || "";
      const aborted =
        requestId && !Object.keys(this._abortCallbacks).includes(requestId);
      if (aborted) {
        content = "";
        break;
      }
      if (text) {
        onStream?.(text);
        content += text;
      }
    }
    if (requestId) {
      delete this._abortCallbacks[requestId];
    }
    if (trace && this.traceOutput) {
      this._logger.log(`âœ… Answer: ${content ?? "None"}`.trim());
    }
    return withDefault(content, undefined);
  }
}

export const openai = new OpenAIClient();
